{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1u4_u0GqUjlElLZkfXIUAXETpEjMfHkb6",
      "authorship_tag": "ABX9TyOFbY401pOzXbk5esaT1EyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parikhshyamal1993/vegetable_classification/blob/main/Vegitable_Classification_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sf2eLp9muUx",
        "outputId": "e8ca56c8-4fb0-4aaa-9835-35d840685355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"device\",device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_base_location =\"/content/drive/MyDrive/vegetable_images/\"\n",
        "dataset_train_location = os.path.join(dataset_base_location,'train')\n",
        "dataset_test_location = os.path.join(dataset_base_location,'test')\n",
        "dataset_validation_location = os.path.join(dataset_base_location,'validation')"
      ],
      "metadata": {
        "id": "DTC5o5fbmw3k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = dataset_base_location\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=256,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLcyDAHyoNtB",
        "outputId": "41d107f4-362f-421b-99fa-cdb0029d0db8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet_18(nn.Module):\n",
        "\n",
        "    def __init__(self, image_channels, num_classes):\n",
        "\n",
        "        super(ResNet_18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        #resnet layers\n",
        "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
        "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
        "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
        "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def __make_layer(self, in_channels, out_channels, stride):\n",
        "\n",
        "        identity_downsample = None\n",
        "        if stride != 1:\n",
        "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
        "\n",
        "        return nn.Sequential(\n",
        "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride),\n",
        "            Block(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def identity_downsample(self, in_channels, out_channels):\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "-dHcNqAGur1X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    if True:\n",
        "        tempdir = '/content/drive/MyDrive/Vegetable_classification_params'\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "                if phase == 'val':\n",
        "                    scheduler.step(epoch_loss)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
        "    return model"
      ],
      "metadata": {
        "id": "MHcOa8l2oNzH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet_18(3,15).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
        "\n",
        "\n",
        "model_ft = train_model(model, criterion, optimizer, lr_scheduler,num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwJehDXinK24",
        "outputId": "6246c06e-ed27-4f7f-80db-4fbcdf8c3ee5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 1.8655 Acc: 0.4219\n",
            "val Loss: 1.0943 Acc: 0.6687\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 0.9694 Acc: 0.7123\n",
            "val Loss: 0.7051 Acc: 0.7823\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 0.6441 Acc: 0.8133\n",
            "val Loss: 0.4662 Acc: 0.8667\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 0.4934 Acc: 0.8595\n",
            "val Loss: 0.3592 Acc: 0.9023\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.3878 Acc: 0.8887\n",
            "val Loss: 0.5771 Acc: 0.7953\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.3279 Acc: 0.9069\n",
            "val Loss: 0.2187 Acc: 0.9350\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.2994 Acc: 0.9137\n",
            "val Loss: 0.2545 Acc: 0.9213\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.2671 Acc: 0.9239\n",
            "val Loss: 0.1440 Acc: 0.9627\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.2358 Acc: 0.9301\n",
            "val Loss: 0.1716 Acc: 0.9543\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.2209 Acc: 0.9355\n",
            "val Loss: 0.1338 Acc: 0.9607\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.1995 Acc: 0.9415\n",
            "val Loss: 0.1483 Acc: 0.9557\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.1773 Acc: 0.9488\n",
            "val Loss: 0.1627 Acc: 0.9467\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.1799 Acc: 0.9476\n",
            "val Loss: 0.0843 Acc: 0.9777\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.1603 Acc: 0.9541\n",
            "val Loss: 0.2112 Acc: 0.9317\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.1491 Acc: 0.9577\n",
            "val Loss: 0.0869 Acc: 0.9783\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.1487 Acc: 0.9565\n",
            "val Loss: 0.0892 Acc: 0.9720\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.1511 Acc: 0.9552\n",
            "val Loss: 0.1029 Acc: 0.9683\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.1298 Acc: 0.9631\n",
            "val Loss: 0.0505 Acc: 0.9867\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.1289 Acc: 0.9618\n",
            "val Loss: 0.0683 Acc: 0.9800\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.1236 Acc: 0.9624\n",
            "val Loss: 0.0515 Acc: 0.9860\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.1151 Acc: 0.9667\n",
            "val Loss: 0.0808 Acc: 0.9757\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.1134 Acc: 0.9676\n",
            "val Loss: 0.0554 Acc: 0.9890\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.1101 Acc: 0.9680\n",
            "val Loss: 0.0661 Acc: 0.9833\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.1039 Acc: 0.9709\n",
            "val Loss: 0.0410 Acc: 0.9907\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.1060 Acc: 0.9679\n",
            "val Loss: 0.0833 Acc: 0.9753\n",
            "\n",
            "Training complete in 47m 18s\n",
            "Best val Acc: 0.990667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model Without Torch modules for process understanding\n"
      ],
      "metadata": {
        "id": "BIgP0I2uuwvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = os.listdir(dataset_train_location)\n",
        "label_map = {i:k for i , k in enumerate(labels)}\n",
        "print(label_map)"
      ],
      "metadata": {
        "id": "nh9KqWpAul6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train = CustomDataLoader(dataset_train_location)\n",
        "test = CustomDataLoader(dataset_test_location)\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "train_features,train_labels = next(iter(train_dataloader))\n",
        "print(train_features)\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "iWMyhRdcnK5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataLoader(Dataset):\n",
        "    def __init__(self,folder,transform=None, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.img_root_folder =folder\n",
        "        self.labels = os.listdir(dataset_train_location)\n",
        "        self.label_map = {i:k for i , k in enumerate(self.labels)}\n",
        "        self.image_dataset = self.data_reader()\n",
        "        self.transform = transform\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "        self.target_transform = target_transform\n",
        "    def data_reader(self):\n",
        "        data_list =[]\n",
        "        for labels , image_type  in self.label_map.items():\n",
        "            folder_path = os.path.join(self.img_root_folder,image_type)\n",
        "            print(folder_path)\n",
        "            for image_name in os.listdir(folder_path):\n",
        "                data_list.append([os.path.join(folder_path,image_name),labels])\n",
        "        return data_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_dataset)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img_path , label = self.image_dataset[idx]\n",
        "        #image = read_image(img_path)\n",
        "        im = (cv2.imread(img_path)[:,:,::-1])\n",
        "        im = cv2.resize(im, (224,224))\n",
        "        im = torch.tensor(im/255)\n",
        "        im = im.permute(2,0,1)\n",
        "        im = self.normalize(im)\n",
        "        return im.float().to(device), torch.tensor([label]).float().to(device)\n",
        ""
      ],
      "metadata": {
        "id": "tjAsJ4f-uSpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model = ResNet_18(3,15).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
        "\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    phase = 'train'\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        inputs, labels = data\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            #print(predicted, labels.flatten())\n",
        "            optimizer.zero_grad()\n",
        "            #print(outputs.size(),labels.flatten().size())\n",
        "            loss = criterion(outputs, labels.flatten().long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "    print(f'Epoch {epoch} loss: {running_loss / len(trainloader)}')"
      ],
      "metadata": {
        "id": "aB-eGDxIuSrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBzjKUjsuSvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7Bu2qYZuSxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQHUmWV0uSzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}